name: "generate_llama"
dump_dir: "./apps/mtp/dump"
# metric_log_dir: str OPTIONAL
ckpt_dir: "./apps/mtp/llama_babylm_lr_min/checkpoints/0000009000"


generator:
  # main.generate.PackedCausalTransformerGeneratorArgs
  temperature: 1.0
  top_p: 0.95
  max_gen_len: 512 # Maximum generation length
  max_tokens: 512 # Maximum number of tokens that can go through the model
  dtype: bf16

single_prompts: ["Amanda was", "I am", "Lisa is"]

  
# harness:
#   tasks:
#     - hellaswag
#     - task: boolq
#       dataset_kwargs:
#         trust_remote_code: true


